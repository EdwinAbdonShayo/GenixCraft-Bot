# Trial 5 of GC Bot
## üì¶ Starting Point: Original System Overview
A set of files that implemented a grid-based robotic arm system using:

* `pose_grid.json`: a manually created map of pixel positions ‚Üí servo angles.

* Manual calibration: `grid_calibration.py`, `pose.py`

* QR-code detection: `main.py`

* Control logic: `arm_control.py`, `ik_controller.py`

* You used `OpenCV` and `pyzbar` for vision, and `Arm_Lib` for controlling the servos.

**Workflow**:
QR is detected ‚Üí center pixel is mapped to nearest pixel in grid ‚Üí corresponding servo angles are sent to move arm ‚Üí grip ‚Üí place.

üí° New Goal: Your Vision-Based IK-Free Approach
You wanted to **ditch the grid-based inverse kinematics** and instead:

    ‚ÄúUse the camera mounted on the robot arm to center the object (with QR code) in the frame, then grab it.‚Äù

This led to the idea of **visual servoing**:

- Use **real-time camera feedback** to steer the robot arm until the object is centered in the camera view.

- No need for pixel‚Üípose lookup or calibration grid.

- Simpler, smarter, adaptable to object drift or new environments.

## ‚ú® Key Changes + What We Built
### ‚úÖ `main.py` ‚Äì Rewritten
- Completely overhauled to implement a visual servoing loop.

- Detects the target object by QR.

- Computes the offset between object center and frame center.

- Moves the robot arm (via `step_move()`) until the object is centered.

- Then sends a "grab" command (calls `grip()`).

- Replaces calibration and IK logic.

### ‚úÖ arm_control.py ‚Äì Enhanced
- New function: `step_move(direction)`

    - Micro-adjusts the robot based on camera feedback.

    - Directions: `"left"`, `"right"`, `"up"`, `"down"`, `"forward"`, `"back"`.

- New function: `sanitize_pose(pose)`

    - Applies safety bounds (e.g., elbow servo must be ‚â•50 if shoulder <40).

    - Keeps all servos within valid [0, 180] range.

- Added support for your robot‚Äôs specific servo roles:

    - Servo 1 = base (X-axis)

    - Servo 2‚Äì4 = Y-axis (reach, depth)

    - Servo 5 = alignment (ignored)

    - Servo 6 = clamp (handled via `grip()`)

### ‚úÖ `pose.py` ‚Äì Reused
- Still used for reading current servo pose in real time.

- Feeds `step_move()` the latest data from hardware.

### ‚ùå Deprecated Files (No Longer Needed)
grid_calibration.py, pose_grid.json, ik_controller.py, test_ik.py, etc.

These were part of the old grid-based approach.

They're still useful for other styles, but not used in this new workflow.

## üß† Final Architecture (as of now)
    [Camera Feed]
        ‚Üì
    [Detect QR Code]
        ‚Üì
    [If it's the target]
        ‚Üì
    [Compute QR position vs frame center]
        ‚Üì
    [Use step_move() to adjust robot in X/Y until QR is centered]
        ‚Üì
    [Send move forward command ‚Üí Grip]
        ‚Üì
    [Done]

## üöÄ What's Next (If You Want)
Implement smoother movement using a proportional controller (based on how far the QR is from center).

Add support for automatic Z-lowering before the grab (maybe based on current servo combo or object type).

Optional: reintroduce depth estimation using QR size or ArUco markers.


"""
GenixCraft Bot ‚Äì Custom Visual Inverse Kinematics

This system implements a visual servoing approach to inverse kinematics,
where the camera detects and tracks a target object (QR code) and adjusts
the robot arm joint angles in real-time to visually align the object to 
the center of the camera frame. 

Once centered, the robot moves forward and grips the object.

No external IK libraries or precomputed angle maps are used.
All motion is generated by custom logic and visual feedback.
"""



[Captured Pose] Current: [90, 90, 0, 10, 90, 50]
[center-left]  [103, 75, 0, 40, 104, 49]
[center-right]  [83, 75, 0, 40, 80, 49]
[bottom-center]  [89, 75, 0, 25, 89, 49]
[top-center]  [90, 75, 0, 45, 89, 49]
[center]  [90, 75, 0, 30, 89, 49]
[top-right]  [85, 75, 0, 45, 80, 49]
[top-left]  [105, 75, 0, 45, 104, 49]
[before-move]  [90, 120, 0, 50, 90, 50add]
